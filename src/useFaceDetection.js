import { useEffect, useRef, useState } from 'react';

export function useFaceDetection(videoRef, enabled) {
  const [faceBox, setFaceBox] = useState(null);
  const detectorRef = useRef(null);
  const intervalRef = useRef(null);

  useEffect(() => {
    if (!enabled || !videoRef.current) return;

    // MediaPipe Face Detectionã®åˆæœŸåŒ–
    const initDetector = async () => {
      if (!window.FaceDetection) {
        console.log("MediaPipe Face Detectionèª­ã¿è¾¼ã¿ä¸­...");
        setTimeout(initDetector, 1000);
        return;
      }

      const faceDetection = new window.FaceDetection({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/${file}`;
        }
      });

      faceDetection.setOptions({
        model: 'short',
        minDetectionConfidence: 0.5
      });

      faceDetection.onResults((results) => {
        if (results.detections && results.detections.length > 0) {
          const detection = results.detections[0];
          const bb = detection.boundingBox;
          
          // æ­£è¦åŒ–ã•ã‚ŒãŸåº§æ¨™ã§é¡ã®é ˜åŸŸã‚’è¨ˆç®—
          const foreheadBox = {
            x: bb.xCenter - bb.width * 0.3,
            y: bb.yCenter - bb.height * 0.45,
            w: bb.width * 0.6,
            h: bb.height * 0.2
          };
          
          setFaceBox(foreheadBox);
          console.log("ðŸ‘¤ é¡”æ¤œå‡º: é¡ã®ä½ç½®ã‚’æ›´æ–°", foreheadBox);
        }
      });

      detectorRef.current = faceDetection;
      console.log("âœ… MediaPipe Face DetectionåˆæœŸåŒ–å®Œäº†");

      // å®šæœŸçš„ã«æ¤œå‡ºã‚’å®Ÿè¡Œ
      intervalRef.current = setInterval(async () => {
        if (videoRef.current && videoRef.current.readyState === 4) {
          await faceDetection.send({ image: videoRef.current });
        }
      }, 500); // 0.5ç§’ã”ã¨ã«æ¤œå‡º
    };

    initDetector();

    return () => {
      if (intervalRef.current) {
        clearInterval(intervalRef.current);
      }
    };
  }, [enabled, videoRef]);

  return faceBox;
}